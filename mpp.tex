%!TEX root = ./master.tex

%\section{McCulloch-Pitts Process}

\subsection*{Motivation}

\annotation{I'm placing the connections to machine and RNNs here, seems weird to have it in simulation section.}

The artificial neural networks that we have in deep learning today are inspired by biological neurons in mammalian brains. Biological neurons are electrical excitable, where a neuron spikes and discharges electrical signals through their synapses, which are the complex membrance junctions that transmit signals to other neurons. An example given in Figure \ref{fig:feedforwardnn} is the feedforward neural network, where in the case of computer vision, we feed the network with an image to do classification of the image into one of the many classes.


\begin{figure}[h]
\centering

\def\layersep{1.8cm}

\begin{tikzpicture}[scale = 1,draw=black!50, node distance=\layersep]
%    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=10pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=blue!70!red!70];
        \tikzstyle{hidden neuron 0}=[neuron, fill=blue!50!red!50];
    \tikzstyle{hidden neuron 1}=[neuron, fill=red!80];
    \tikzstyle{hidden neuron 2}=[neuron, fill=red!50];
        \tikzstyle{output neuron}=[neuron, fill=orange!70];

    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,6}
    	\node[input neuron] (I-\name) at (0,-\y) {};
		\node[input neuron] (I-4) at (0,-4) {};

 \foreach \name / \y in {1,...,6}
    	            \node[hidden neuron 0] (B-\name) at (\layersep,-\y cm) {};


    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,6}
            \node[hidden neuron 1] (H-\name) at (2*\layersep,-\y cm) {};
    % Draw the output layer node
%    \node[output neuron,pin={[pin edge={->}]right:$h_{W,b}(x) = f(z_{1}^{(3)}) = a_{1}^{(3)}$}, right of=H-3] (O) {};
 \foreach \name / \y in {1,...,6}
            \node[hidden neuron 2] (P-\name) at (3*\layersep,-\y cm) {};

 \foreach \name / \y in {1,...,3}
 \path[yshift=-1.5cm]
      node[output neuron] (O-\name) at (4*\layersep,-\y cm) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,6}
        \foreach \dest in {1,...,6}
            \path[color = blue!70!red!70] (I-\source) edge (B-\dest);

  \foreach \source in {1,...,6}
        \foreach \dest in {1,...,6}
            \path[color = blue!50!red!50] (B-\source) edge (H-\dest);

\foreach \source in {1,...,6}
        \foreach \dest in {1,...,6}
            \path[color = red!80] (H-\source) edge (P-\dest);

\foreach \source in {1,...,6}
        \foreach \dest in {1,...,3}
            \path[color = red!50] (P-\source) edge (O-\dest);

%    \node[annot,below=0.25cm of {H-6}, node distance=1cm] (hl2) {\small {\color{red!80}L3}};
%    \node[annot,left of ={hl2}](hl1) {\small {\color{blue!50!red!50}L2}};
%        \node[annot,left of=hl1] {\small {\color{blue!70!red!70}L1}};
%            \node[annot,right of=hl2](hl3) {\small {\color{red!50}L4}};
%            \node[annot,right of=hl3] {\small {\color{orange!70}L5}};
\node[left=1.25cm of {I-3},yshift=-.5cm] (dog) {};
 \foreach \dest in {1,...,6}
            \draw[color = black!30,->,>=stealth, decoration, decoration={snake}] (dog) -- (I-\dest);

            \filldraw [black!50,scale=0.1,transform node coordinates,shift={(-25,-45)}]
  plot  coordinates \dogcoordinates  -- cycle;
  
%  \node[output neuron] at (O-2) {{\color{white}1}};
%    \node[hidden neuron 0] at (B-2) {{\color{white}1}};
%        \node[hidden neuron 1] at (H-4) {{\color{white}1}};
%            \node[hidden neuron 2] at (P-6) {{\color{white}1}};
%             \node[input neuron] at (I-4) {{\color{white}1}};
             \path[color = blue!70!red!70, very thick] (I-4) edge (B-2);
           \path[color = blue!50!red!50, very thick] (B-2) edge (H-4);
            \path[color = red!80, very thick] (H-4) edge (P-6);
      \path[color = red!50, very thick] (P-6) edge (O-2);
      \node[right=0.25cm of {O-2}](w) {{\color{orange!70}``Dog''}};
            \draw[->,>=stealth, color=orange!70,very thick] (O-2) -- (w);
\end{tikzpicture}
\caption{Feedforward Neural Network}
\label{fig:feedforwardnn}
\end{figure}

However, in a feedforward network, the connection of the different layers of the neuron do not form a cycle. This is not true for biological neurons as there is no one direction of the connection of the neurons. Thus we look to recurrent neural networks (RNN), where the connections between the neurons to form a directed cycle is allowed. One of the popular RNNs is long short term memory (LSTM), which are able to connect previous information to the present task. The neurons in LSTM communicate with real values, which is different from how biological neurons communicate in mammalian brains. Therefore, we only allow the neurons in our network to be binary-valued, $\{0,1\}$ where a transition $0 \to 1$ represents a spiking activity and a transition $1 \to 0$ represents the recovery of a neuron to an armed state.


\subsection*{The statistical model}
%\annotation{Will remove subsections and make sections instead}

Given a directed graph $G=(V,E)$ without multiple edges, with vertex weights $\beta_i > 0$ and edge weights $\alpha_{ij} > 0$, a \emph{McCulloch-Pitts process (MPP)} is an activity-based process with binary states $x \in \{0,1\}^{|V|}$ and transitions $xy$ where state $y$ is one-bit away from state $x$. If $y$ and $x$ differs in the $i$-th bit, we define the transition rate
\begin{align*}
F_{xy} = \left[ \beta_i^{\sigma_i} \alpha_i^{x \sigma_i} \right]^{1/\tau}
%\text{\annotation{ what is $\sigma_i$?}}
\end{align*}
where $\alpha_i^{x} = \alpha_{1i}^{x_1}\alpha_{2i}^{x_2}\ldots\alpha_{di}^{x_{d}}$ with $|V|=d$ and $\sigma_i = 1 - 2x_i \in \{-1,+1\}$ denotes the change in the state of the $i$-th bit.


\begin{figure}[h]
\centering
        \begin{tikzpicture}[scale = 2,-,draw=black!50, node distance=\layersep,>=stealth]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt];
    \tikzstyle{unit1}=[neuron, fill=red!50,thick,];
        \tikzstyle{unit2}=[neuron, fill=blue!50,thick,];
            \tikzstyle{unit3}=[neuron, fill=green!50,thick,];
 \def \radius {1cm}
 \def \n {3}
 \foreach \s in {1,...,\n}{
  \node[unit] (\s) at ({360/\n * (\s - 1) - 180}:\radius) {};
}  
   \DoubleLine{1}{2}{<-,draw=black!50}{}{->,draw=black!50}{};
   \DoubleLine{1}{3}{<-,draw=black!50}{}{->,draw=black!50}{};
    \DoubleLine{2}{3}{<-,draw=black!50}{}{->,draw=black!50}{};
      \node[unit3](k) at ({60}:\radius) {{\color{white}$x_3$}};
  \node[unit1](i) at ({180 }:\radius) {{\color{white}$x_1$}};
    \node[unit2](j) at ({300 }:\radius) {{\color{white}$x_2$}};
    \draw [->,draw=black!50, thick] (1.125) arc (20:300:1.5mm) node[pos=-0.5,left] {} (1);
\draw [->,draw=black!50, thick] (2.250) arc (-215:70:1.5mm) node[pos=-0.5,left] {} (2);
\draw [->,draw=black!50, thick] (3.15) arc (-85:195:1.5mm) node[pos=-0.5,left] {} (3);

    \node[below =.005cm of 1] {$\beta_1$};
%        \node[above left=.05cm of 3] {$\beta_3$};
            \node[right=.005cm of 2] {$\beta_2$};
    \node at (-.4,-.55) {$\alpha_{12}$};
     \node at (-.2,-.25) {$\alpha_{21}$};  
          \node at (-1.5,0.1) {$\alpha_{11}$};    
%\node at (1.255,0.8) {$w_{21}$};
%\node at (2.525,-0.1){$w_{11}$};
\end{tikzpicture}
\caption{McCulloch-Pitts process with three neurons.}
\label{fig:mpp}
\end{figure}

Working with the example in Figure \ref{fig:mpp} and choosing $\tau = 1$, the transition rate matrix is given by

\begin{align*}
    \scalemath{0.7}{
F=\kbordermatrix{
          & 000 & 001 & 010 & 011 & 100 & 101 & 110 & 111 \\
    000 &\ast  & \beta_3 & \beta_2 & 0 & \beta_1 & 0 & 0 & 0 \\
    001 & \beta_3^{-1}\alpha_{33}^{-1} & \ast & 0 & \beta_2\alpha_{32} & 0 & \beta_1\alpha_{31} & 0 & 0  \\
    010 & \beta_2^{-1}\alpha_{22}^{-1} & 0 &\ast  & \beta_3\alpha_{23} & 0 & 0 & \beta_1\alpha_{21} & 0  \\
    011 & 0 & \beta_{2}^{-1}\alpha_{22}^{-1}\alpha_{32}^{-1} & \beta_3^{-1}\alpha_{23}^{-1}\alpha_{33}^{-1} &\ast  & 0 & 0 & 0 & \beta_{1}\alpha_{21}\alpha_{31}  \\
    100 & \beta_1^{-1}\alpha_{11}^{-1} & 0 & 0 & 0 & \ast & \beta_3\alpha_{13} & \beta_{2}\alpha_{12} & 0  \\
    101 & 0 & \beta_1^{-1}\alpha_{11}^{-1}\alpha_{31}^{-1} & 0 & 0 & \beta_{3}^{-1}\alpha_{13}^{-1}\alpha_{33}^{-1} & \ast & 0 & \beta_2\alpha_{12}\alpha_{32}  \\
    110 & 0 & 0 & \beta_1^{-1}\alpha_{11}^{-1}\alpha_{21}^{-1} & 0 & \beta_{2}^{-1}\alpha_{12}^{-1}\alpha_{22}^{-1} & 0 & \ast & \beta_3\alpha_{13}\alpha_{23}  \\
    111 & 0 & 0 & 0 & (\beta_1\alpha_1^{111})^{-1} & 0 & (\beta_2\alpha_{2}^{111})^{-1} & (\beta_3\alpha_{3}^{111})^{-1} & \ast  \\
  }}
\end{align*}

where $\ast$ denotes the negative of the sum of its corresponding row, and $\alpha_i^{111} = \alpha_{1i}\alpha_{2i}\alpha_{3i}$. 

\subsection*{Simulation}

The simulation of the McCulloch-Pitts process starts by drawing an inital state $x^{(0)}$  from a distribution $p^{(0)}$. Then for any state $x$, it holds for some time 
\begin{align*}
\Delta t \sim \text{Exp}(\lambda_x)
\end{align*}
where $\lambda_x = \sum_{y \neq x}F_{xy}$ and it transits to state $y$ which is one hop away with probability 
\begin{align*}
F_{xy}/\lambda_x
\end{align*}
Thus the temporal data obtained from the simulation are binary tuples of length $|V|$ and an associated holding time for each pair of consecutive state. In Figure \ref{fig:simulateddata} we give an example of the data that is obtained from simulating the MPP.

\begin{figure}[h]
\centering
\begin{python}
0000: state [1 1 0 1 1 1 1 0] | neuron 002 | holding time 0.0032901008027207005
0001: state [1 1 1 1 1 1 1 0] | neuron 003 | holding time 0.16460101982700073
0002: state [1 1 1 0 1 1 1 0] | neuron 007 | holding time 0.019009826806025597
0003: state [1 1 1 0 1 1 1 1] | neuron 003 | holding time 0.13344528911418524
0004: state [1 1 1 1 1 1 1 1] | neuron 006 | holding time 0.03776971154447096
0005: state [1 1 1 1 1 1 0 1] | neuron 007 | holding time 0.003934267162875663
0006: state [1 1 1 1 1 1 0 0] | neuron 006 | holding time 0.38924525541203236
0007: state [1 1 1 1 1 1 1 0] | neuron 006 | holding time 0.011087533081789665
\end{python}
\label{fig:simulateddata}
\caption{Temporal data generated from simulation for a MPP with eight neurons}
\end{figure}




%\annotation{Could you write something about the connection to machine learning/recurrent neural networks? Thanks! }

%\subsection*{Toric Variety}
%\annotation{I would remove this section if this is ok. I wouldn't redefine what it means to be a toric variety etc, as we want to keep this short.}
%
%With the statistical model defined proper, we would like to show that given directed graph $G = (V,E)$  with $|V| = d$, with non-negative vertex and edge weights $\beta_i, \alpha_{ij}$ respectively, we have a map $F: \mathbb{C}^{d(d+1)} \to \mathbb{C}^{d\cdot 2^d}$ defined by $(\beta_i,\alpha_{ij})\mapsto (F_{xy})$, from the space of non-negative parameters to the space of stochastic space matrices and the closure(note: do we really need to take its closure or it is guaranteed to be close?) of the image is a toric variety.
%
%The Zariski closure of $X \subset \mathbb{C}$, is
%\begin{align*}
%\overline{X}:=\{p \in \mathbb{C}^n \mid f(p) = 0 \text{ whenever } f(X) = 0\}
%\end{align*}
%
%
%\begin{defn}
%A toric variety is an irreducible variety $V$ such that
%\begin{itemize}
%\item $(\mathbb{C}^\ast)^n$ is a Zariski open subset of $V$
%\item the action of $(\mathbb{C}^\ast)^n$ on itself extends to an action of $(\mathbb{C}^\ast)^n$ on $V$.
%\end{itemize}
%\end{defn}
%
%Next we would like know the degree and dimension of our toric variety. By first principles, we can obtain them by.... 
%
%
%{\color{blue}I would like to give some heuristics here, like the degree is the number of interections when we stab the toric variety with a line, plane or in general with a hyperplane. What good does knowing the dimension and degree helps us with and how the method of finding the dimension we used on Wednesday is equivalent to finding the dimension and degree by the method we discuss above.}

\subsection*{The Toric Variety}

We consider the space of weights $W \coloneqq \mathbb C^{d+|E|} = \{(\beta_{i},\alpha_{jk})\mid i\in V,(j,k)\in E\}$ and the space of transition rates $T \coloneqq \mathbb C^{2^{d}d} = \{(F_{xy}) \mid x,y \text{ binary states differing at one bit}\}$. We have a map
$f\colon W\to T$ defined by $f(\alpha,\beta) = (F_{xy}(\alpha,\beta))_{xy}$. We define the toric variety $X$ as the Zariski closure of the image of $f$.

In the above example, we get the map $f\colon \mathbb C^{12}\to \mathbb C^{24}$ and the induced toric variety $X$. Using Polymake, we can compute the Hilbert series of (the closure in $\mathbb P^{24}$ of) $X$. We obtain The Hilbert series
	\[\frac{P(x)}{(1-x)^{12}}\]
with \[P(x)=x^6 + 12x^5 + 51x^4 + 88x^3 + 51x^2 + 12x + 1.\]

The dimension of $X$ is thus $12$, i.\ e.\ the degree of the denominator, and the degree of X is $P(1)=216$.

\subsection*{A group action}
Let $\pi$ be a graph isomorphism of $G$, seen as a map $V\to V$. The map $\pi$ acts on the space of weights by sending an element $(\beta_i,\alpha_{j,k})$ to $(\beta_{\pi i},\alpha_{\pi{j},\pi{k}})$, and similarly it acts on the space of transition rates. Since $f$ is a Laurent map, for all weights $(\alpha,\beta)$ we have $\pi f (\alpha,\beta) = f \pi (\alpha, \beta)$. Hence we obtain a group action of $\operatorname{Aut}(G)$ on the variety $X$.

In the above example we see for instance that the permutation group $S_6$ acts on $X$.